# üöÄ BusinessThis Ollama Deployment Checklist

## ‚úÖ Pre-Deployment Checklist

### 1. System Requirements
- [ ] **RAM**: 6GB+ available (8GB recommended)
- [ ] **Storage**: 4GB+ free space for Mistral model
- [ ] **CPU**: Modern multi-core processor
- [ ] **OS**: Windows 10/11, macOS, or Linux

### 2. Ollama Installation
- [ ] **Download Ollama**: https://ollama.com/download
- [ ] **Install Ollama** on your system
- [ ] **Start Ollama service**: `ollama serve`
- [ ] **Pull Mistral model**: `ollama pull mistral`
- [ ] **Verify installation**: `ollama list` shows `mistral`

### 3. Environment Configuration
- [ ] **Create `.env` file** with Ollama settings:
  ```env
  OLLAMA_URL=http://localhost:11434
  OLLAMA_MODEL=mistral
  ```
- [ ] **Install Python dependencies**: `pip install -r requirements/integrations.txt`
- [ ] **Verify requests library**: `python -c "import requests"`

### 4. Testing
- [ ] **Run test suite**: `python test_ollama_integration.py`
- [ ] **Test Ollama API**: `curl http://localhost:11434/api/tags`
- [ ] **Test Mistral model**: `ollama run mistral "Hello"`
- [ ] **Verify health endpoint**: `curl http://localhost:5000/api/health/ollama`

## üöÄ Deployment Steps

### Step 1: Install Ollama

**Windows:**
```cmd
# Download and install
Invoke-WebRequest -Uri https://ollama.com/download/windows -OutFile ollama-windows-amd64.exe
ollama-windows-amd64.exe

# Start service
ollama serve

# Pull model
ollama pull mistral
```

**Linux/macOS:**
```bash
# Install
curl -fsSL https://ollama.com/install.sh | sh

# Start service
ollama serve &

# Pull model
ollama pull mistral
```

### Step 2: Configure Environment

Create or update your `.env` file:
```env
# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=mistral

# Your existing environment variables
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
# ... other variables
```

### Step 3: Install Dependencies

```bash
# Install Python dependencies
pip install -r requirements/integrations.txt

# Verify requests is installed
python -c "import requests; print('‚úÖ requests installed')"
```

### Step 4: Test Integration

```bash
# Run comprehensive test suite
python test_ollama_integration.py

# Expected output: All tests should pass
# If tests fail, check Ollama is running
```

### Step 5: Start Application

```bash
# Terminal 1: Start backend
python run_backend.py

# Terminal 2: Start frontend
python run_frontend.py

# Terminal 3: Keep Ollama running
ollama serve
```

## üîç Verification Steps

### 1. Health Checks

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Check BusinessThis health
curl http://localhost:5000/api/health

# Check Ollama integration
curl http://localhost:5000/api/health/ollama
```

**Expected responses:**
- Ollama: `{"models":[...]}`
- Backend: `{"status":"healthy"}`
- Integration: `{"ollama_status":"healthy"}`

### 2. AI Feature Tests

Test each AI endpoint:
```bash
# Financial coaching
curl -X POST http://localhost:5000/api/ai/coaching \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{"question":"How can I save money?"}'

# Daily tip
curl -X GET http://localhost:5000/api/ai/daily-tip \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"

# Spending recommendations
curl -X GET http://localhost:5000/api/ai/spending-recommendations \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

### 3. Performance Verification

- [ ] **Response time**: 1-3 seconds per AI query
- [ ] **Memory usage**: <6GB RAM for Ollama
- [ ] **CPU usage**: Reasonable during inference
- [ ] **Error handling**: Graceful fallbacks when Ollama unavailable

## üõ† Troubleshooting

### Common Issues

1. **"Connection refused" to Ollama**
   ```bash
   # Check if Ollama is running
   ollama ps
   
   # Start Ollama if not running
   ollama serve
   ```

2. **"Model not found" error**
   ```bash
   # Pull the model
   ollama pull mistral
   
   # List available models
   ollama list
   ```

3. **Slow responses**
   - Check CPU usage during inference
   - Ensure adequate RAM available
   - Consider upgrading server specs

4. **Out of memory**
   - Close other applications
   - Ensure 6GB+ RAM available
   - Consider smaller model if needed

### Performance Optimization

1. **Keep Ollama running**
   - Don't restart between requests
   - Models stay in memory for faster responses

2. **Monitor resources**
   ```bash
   # Check Ollama status
   ollama ps
   
   # Monitor memory usage
   # Use Task Manager (Windows) or htop (Linux)
   ```

3. **Upgrade if needed**
   - More RAM for larger models
   - GPU acceleration for faster inference
   - Multiple Ollama instances for load balancing

## üìä Success Metrics

### Technical Metrics
- [ ] **Uptime**: Ollama service running 99%+ of time
- [ ] **Response time**: <3 seconds average
- [ ] **Error rate**: <1% of AI requests fail
- [ ] **Memory usage**: Stable, no memory leaks

### Business Metrics
- [ ] **Cost savings**: $240-600/year eliminated
- [ ] **User satisfaction**: AI features working smoothly
- [ ] **Feature adoption**: Users engaging with AI features
- [ ] **Performance**: No user complaints about speed

## üéØ Post-Deployment

### Monitoring Setup
1. **Health monitoring**: Set up alerts for Ollama downtime
2. **Performance tracking**: Monitor response times
3. **Usage analytics**: Track AI feature adoption
4. **Error logging**: Monitor and fix any issues

### Maintenance
1. **Regular updates**: Keep Ollama and models updated
2. **Resource monitoring**: Watch CPU/RAM usage
3. **Backup strategy**: Backup model files if needed
4. **Scaling plan**: Prepare for increased usage

## üéâ Success!

Once all checklist items are complete:

‚úÖ **Zero AI API costs**  
‚úÖ **Unlimited AI features**  
‚úÖ **Enhanced user experience**  
‚úÖ **Improved competitive position**  
‚úÖ **Better privacy and security**  

Your BusinessThis application is now powered by free, local AI! üöÄ

---

**Need Help?**
- Check `OLLAMA_SETUP.md` for detailed setup
- Run `python test_ollama_integration.py` to verify
- Monitor with `curl http://localhost:5000/api/health/ollama`
